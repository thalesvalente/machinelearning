{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","version":"0.3.2","provenance":[{"file_id":"1RXF1ieP012Fh06z9kYyewrly3980AOWY","timestamp":1561083315191}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ToQxgBUZDOO2","colab_type":"text"},"source":["# Redes Neurais Convolucionais\n","Nesse tutorial vamos falar sobre um tipo diferente de redes neurais: as redes neurais convolucionais (CNNs, do inglês Convolutional Neura Networks).\n","\n","Um dos usos mais populares para CNNs é (discutivelmente) o processamento de imagens.  Elas são usadas pra reconhecer o tipo da imagem, encontrar objetos espefícicos nela, desenhar o contorno desses objetos e até gerar novas imagens com base em outras.\n","\n","Nessa aula vamos aprender como funciona as CNNs e como usá-as para reconhecer o tipo da imagem. Em outras palavras, vamos fazer **classificação de imagens**. Para isso, usaremos uma nova biblioteca: **Keras**. Keras é certamente uma das bibliotecas mais simples e mais usadas para criar CNNs e outros tipos de deep learning e por isso usaremos para facilitar o aprendizado.\n"]},{"cell_type":"markdown","metadata":{"id":"SfC8oA4qHBL_","colab_type":"text"},"source":["# 1. Por que classificar imagens é complicado?\n","Nós, seres humanos, num piscar de olhos, conseguimos identificar os objetos que nos rodeiam. Para isso, nosso cérebro se basea em experiências visuais passadas e generaliza para novas, mesmo que os objetos estejam em cores, formas e posições diferentes, em ambientes novos e com iluminação diferente. Infelizmente, para as máquinas não é tão simples assim.\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"O que vemos e o que o computador vê\" />\n","      <center>\n","        <img  alt=\"O que vemos e o que o computador vê\" src=\"https://adeshpande3.github.io/assets/Corgi3.png\">\n","        <figcaption> O que vemos e o que o computador vê. Fonte: <cite data-cite=\"Deshpande2017\"><a href=\"https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/\">(Deshpande, 2017)</a>\n","        </figcaption>\n","      </center>\n","</figure>\n","\n","Quando um computador vê uma imagem, ele verá na verdade uma matriz de valores de pixel. Esses números, embora para nós possam não fazer sentido, são as únicas entradas disponíveis para o computador. Pior ainda: as menores mudanças na imagem, sejam em posição, forma, cor, iluminação... podem mudar completamente esses números."]},{"cell_type":"markdown","metadata":{"id":"a6OKuNhsX7bV","colab_type":"text"},"source":["# 2. Como as CNNs podem ajudar?\n","Sabendo que o ser humano classifica bem as imagens, as CNNs buscaram inspiração em nosso sistema visual. Quando olhamos para uma foto de um cachorro, nós o identificamos ao perceber que ele tem quatro patas, orelhas, focinho, etc. De maneira semelhante, uma CNN é capaz de fazer isso procurando por padrões simples, como bordas e curvas. Então ela combina esses padrões para formar outros mais complexos, como formas e texturas e depois combina esses em outros ainda mais complexos... e logo temos padrões complexos o suficiente para identificar o cachorro. \n","\n","Para a CNN aprender esses padrões, a primeira coisa que precisamos é de um dataset. Vamos utilizar o dataset mnist, que contém imagens de dígitos. Em aprendizado supervisionado de classificação, cada amostra de um dataset precisa de um valor que a identifique (label). No mnist não é diferente: cada imagem possui uma label que diz qual dígito está desenhado na imagem. "]},{"cell_type":"code","metadata":{"id":"JaB6bJftN2z8","colab_type":"code","outputId":"521863b3-360c-448f-8e22-ef9bffcb51f5","executionInfo":{"status":"ok","timestamp":1561972647508,"user_tz":180,"elapsed":4489,"user":{"displayName":"Pedro Diniz","photoUrl":"","userId":"09876185426137863871"}},"colab":{"base_uri":"https://localhost:8080/","height":364}},"source":["# Preparando o dataset para aprendizado de máquina\n","\n","# Baixando o dataset mnist\n","from keras.datasets import mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Dividindo o dataset de treino em treino e validação de forma balanceada\n","from sklearn.model_selection import train_test_split\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, stratify=y_train, test_size=0.25)\n","\n","\n","# Checando quantidade de imagens do dataset\n","print('Quantidade de imagens de treino:', x_train.shape[0])\n","print('Quantidade de imagens de validação:', x_val.shape[0])\n","print('Quantidade de imagens de test:', x_test.shape[0])\n","\n","# Contando quantidade de imagens por dígito\n","import collections\n","counterTrain=collections.Counter(y_train)\n","counterVal=collections.Counter(y_val)\n","counterTest=collections.Counter(y_test)\n","\n","# Plotando quantidade de imagens de cada dígito\n","from matplotlib import pyplot\n","\n","fig, ax = pyplot.subplots()\n","rects1 = ax.bar(counterTrain.keys(), counterTrain.values(), label='Treino')\n","rects2 = ax.bar(counterVal.keys(), counterVal.values(), label='Validação')\n","rects3 = ax.bar(counterTest.keys(), counterTest.values(), label='Teste')\n","\n","ax.set_title('Imagens por dígito')\n","ax.set_ylabel('Quantidade de imagens')\n","ax.set_xlabel('Dígito')\n","ax.legend()\n","pyplot.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Quantidade de imagens de treino: 45000\n","Quantidade de imagens de validação: 15000\n","Quantidade de imagens de test: 10000\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAY4AAAEXCAYAAAC6baP3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"rzydW3R_hx7f","colab_type":"text"},"source":["Agora que temos um dataset, é preciso formatá-lo para que a API do Keras possa entendê-lo."]},{"cell_type":"code","metadata":{"id":"Xj0_y8KCWuIc","colab_type":"code","colab":{}},"source":["# Formatando o dataset para funcionar como entrada do Keras\n","\n","# As imagens de entradas precisam estar em um array de 4 dimensões\n","x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n","x_val = x_val.reshape(x_val.shape[0], 28, 28, 1)\n","x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n","\n","# Cada imagem precisa ter dimensão x, y e z\n","input_shape = (28, 28, 1)\n","\n","# Convertento valores dos pixels para float (garantindo precisão em operações de divisão por exemplo)\n","x_train = x_train.astype('float32')\n","x_val = x_val.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","# Normalizando os valores dos pixels (valores entre 0 e 1).\n","x_train /= 255\n","x_val /= 255\n","x_test /= 255"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sj9irTjAmOVO","colab_type":"text"},"source":["# 3. Montando uma CNN\n","Para encontrar padrões e combiná-los sucessivamente, uma CNN utiliza um conjunto de operações em sequência. Essa operações são explicadas a seguir.\n","\n","## 3.1 Convolução\n","O objetivo principal da convolução é extrair características da imagem de entrada. \n","\n","Considere uma primeira imagem 5 x 5, chamada de entrada. Também considere uma segunda imagem 3 x 3, chamada de filtro. Então a convolução da entrada por um filtro é computada assim:\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"Convolução de uma entrada 5x5 por um filtro 3x3\" />\n","      <center>\n","          <img  alt=\"Convolução de uma entrada 5x5 por um filtro 3x3\" src=\"https://cdn-images-1.medium.com/max/800/0*KdJv2eWBC1qUs3Po\" >\n","          <figcaption> Convolução de uma entrada 5x5 por um filtro 3x3. Fonte: <cite data-cite=\"Agarwal2018\"><a href=\"https://medium.com/nybles/create-your-first-image-recognition-classifier-using-cnn-keras-and-tensorflow-backend-6eaab98d14dd\">(Agarwal, 2018)</a></cite>\n","        </figcaption>\n","      </center>\n","</figure>\n","\n","A imagem 3 x 3 resultante dessa convolução é chamada de mapa de características. Nela, os pixels de maiores valores representam as vizinhanças 3 x 3 da entrada que tiveram os valores mais semelhantes aos valores do filtro. \n","\n","Nesse exemplo, convoluímos com apenas um filtro, mas normalmente aplica-se vários. Se aplicarmos, por exemplo, 5 filtros, nosso mapa de características será composto de 5 imagens 3 x 3, ou uma imagem 3 x 3 x 5.\n","\n","## 3.2 Função de Ativação\n","\n","Normalmente, após a convolução, aplica-se uma função de ativação. A função de ativação mais utilizada em CNNs é a ReLU (rectified linear unit) e é definida como: f (x) = max (0, x). Se x> 0, os valores do mapa de características permanecem iguais, e se x <0, ele corta detalhes desnecessários.\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"Função de ativação ReLU\" />\n","      <center>\n","          <img  alt=\"Função de ativação ReLU\" src=\"https://www.researchgate.net/profile/Leo_Pauly/publication/319235847/figure/fig3/AS:537056121634820@1505055565670/ReLU-activation-function.png\" >\n","      </center>\n","</figure>\n","\n","## 3.3 Pooling\n","O pooling reduz a dimensionalidade do mapa de características, mas retém as informações mais importantes. \n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"Maxpooling 2x2 em uma entrada 4x4\" />\n","      <center>\n","          <img  alt=\"Maxpooling 2x2 em uma entrada 4x4\" src=\"https://cdn-images-1.medium.com/max/800/1*oVOUhBIi59Gb5w7eBzqYuA.png\">\n","          <figcaption> Maxpooling 2x2 em uma entrada 4x4. Fonte: <cite data-cite=\"Yalçin2018\"><a href=\"https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d\">(Yalçin, 2018)</a></cite>\n","        </figcaption>\n","      </center>\n","</figure>\n","\n","Na figura, utilizamos o Max Pooling 2 x 2. Nele, ficamos apenas com os maiores valores de cada vizinhança 2 x 2 dentro da imagem. Mas também podemos obter a média (Average Pooling) ou outra operação que desejarmos. Na prática, o Max Pooling geralmente funcionar melhor.\n","\n","## 3.4 Flatten e MLP\n","Na operação Flatten, o mapa de características é convertido em um vetor. Normalmente, aplica-se essa operação para que o mapa de características possa ser usado como entrada de uma rede neural comum, ou seja, um Multilayer Perceptron (MLP).\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"Flatten seguido de uma MLP\" />\n","      <center>\n","          <img  alt=\"Flatten seguido de uma MLP\" src=\"https://camo.githubusercontent.com/de4ac4d9cea5750785f86b1a8dd7cf511c9d9a3d/68747470733a2f2f61636975732e636f2e756b2f77702d636f6e74656e742f7468656d65732f61636975732f6d616368696e655f6c6561726e696e672f696d67732f646c2f6c617267652d616e6e2d66726f6d2d636e6e2e706e67\">\n","          <figcaption> Flatten seguido de uma MLP. Fonte: <cite data-cite=\"Partridge2018\"><a href=\"https://github.com/Achronus/Machine-Learning-101/wiki/Convolutional-Neural-Networks-(CNN)\">(Partridge, 2018)</a></cite>\n","        </figcaption>\n","      </center>\n","</figure>\n","\n","O MLP se comportará como deve: as usará as características de entrada para aprender e, posteriormente, reconhecer as imagens. Podemos experimentar qualquer de neurônios em suas camadas. No entanto, para a mnist, a camada final deve ter 10 neurônios, pois temos 10 classes numéricas (0, 1, 2,…, 9).\n","\n","## 3.5 Dropout\n","\n","Redes muito complexas tendem a gerar modelos igualmente complexos. Comumente, esses modelos são formados sob forte influência de apenas um subconjunto de neurônios em detrimento dos demais. Em situações como essa, esse subconjunto cria relacionamentos muito complexos, fazendo com que o modelo final sobreajuste (overfit) as imagens de treino. \n","\n","O overfitting ocorre quando o modelo treinado reconhece bem as imagens de treino, mas não tão bem assim as imagens de teste. Para tentar aliviar isso, existem métodos conhecidos como regularizações. Um dos mais utilizados em Deep Learning é o Dropout que desconsidera alguns neurônios no decorrer do treinamento.\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"Dropout de 50%\" />\n","      <center>\n","          <img  alt=\"Dropout de 50%\" src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif\" >\n","          <figcaption>Dropout de 50%. Fonte: <cite data-cite=\"Jain2018\"><a href=\"https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/\">(Jain, 2018)</a></cite>\n","        </figcaption>\n","      </center>\n","</figure>\n","\n","No exemplo da figura, utilizou-se um dropout de 50% em uma camada. Isso significa que, a cada iteração no treinamento, cada neurônio dessa camada terá 50% de chance de ser removido e, assim, não terá influência nas próximas camadas.\n","\n","O dropout evita que apenas um pequeno grupo de neurônios crie relacionamentos muito complexos e que tenham muita influência no modelo final. Para isso, ele força a rede a tentar, a cada iteração, novos relacionamentos entre alguns neurônios ao excluir os demais. Assim, cada neurônio pode contribuir um pouco para o modelo final através de relacionamentos mais simples. \n","\n","## 3.6 SoftMax\n","\n","A função SoftMax converte a saída da CNN em um vetor de valores, onde cada valor é a probabilidade de a entrada da CNN ser de uma essa classe. Por exemplo: uma CNN classifica imagens como cachorro e gato. Ao receber uma imagem de um cachorro, ela infere que essa imagem tem 95% de chance de ser um cachorro e 5% chance de ser um gato.\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"SoftMax\" />\n","      <center>\n","          <img  alt=\"SoftMax\" width=\"17%\" src=\"https://camo.githubusercontent.com/82fca6566989a40f9aebac982c4aa4af765b738f/68747470733a2f2f61636975732e636f2e756b2f77702d636f6e74656e742f7468656d65732f61636975732f6d616368696e655f6c6561726e696e672f696d67732f646c2f736f66746d61782d66756e6374696f6e2e706e67\" >\n","      </center>\n","</figure>\n","\n","Abaixo vamos ver como podemos criar essas operações usando o Keras.\n"]},{"cell_type":"code","metadata":{"id":"DO6KIQkTKAAB","colab_type":"code","outputId":"3c30b888-e7a3-43f6-c2ff-cb10c29e7cc9","executionInfo":{"status":"ok","timestamp":1561972648091,"user_tz":180,"elapsed":4991,"user":{"displayName":"Pedro Diniz","photoUrl":"","userId":"09876185426137863871"}},"colab":{"base_uri":"https://localhost:8080/","height":612}},"source":["# Importando Keras e suas operações\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","\n","\n","# Inicializando a CNN\n","model = Sequential()\n","\n","\n","# Operação de convolução com filtro 3 x 3 seguida da função de ativação ReLU\n","model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape, activation='relu'))\n","\n","# Operação de Max Pooling 2 x 2\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# Operação de convolução com filtro 3 x 3 seguida da função de ativação ReLU\n","model.add(Conv2D(28, kernel_size=(3,3), activation='relu'))\n","\n","\n","# Operação de flatten (convertento o mapa de características em um vetor)\n","model.add(Flatten())\n","\n","\n","# Camada densa com 128 nerônios seguida da função de ativação ReLU\n","model.add(Dense(128, activation='relu'))\n","\n","# Dropout de 50% dos neurônios\n","model.add(Dropout(0.5))\n","\n","# Camada densa de saída com 10 (um para cada dígito) seguida de função SoftMax\n","model.add(Dense(10,activation='softmax'))\n","\n","\n","# Resumo do modelo\n","model.summary();"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0701 09:17:28.277284 140211596105600 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0701 09:17:28.296580 140211596105600 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0701 09:17:28.301336 140211596105600 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0701 09:17:28.322230 140211596105600 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","W0701 09:17:28.370676 140211596105600 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0701 09:17:28.383626 140211596105600 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stderr"},{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 26, 26, 28)        280       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 13, 13, 28)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 11, 11, 28)        7084      \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 3388)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 128)               433792    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                1290      \n","=================================================================\n","Total params: 442,446\n","Trainable params: 442,446\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_EidOeemY9rX","colab_type":"text"},"source":["# 4. Treinando uma CNN\n","\n","Já criamos uma CNN vazia. Agora é hora de usá-la para aprender o dígitos utilizando suas imagens e labels. Primeiro, definiremos um **otimizador** que tentará iterativamente minimizar uma função de **perda**. Essa função de perda compara os valores de dígitos preditos com os respectivos valores das labels. Assim, quanto menor esse valor de perda, mais a rede predirá corretamente. Também definiremos **métrica** de eficiência para podermos dizer o quanto a rede acerta a cada época."]},{"cell_type":"code","metadata":{"id":"t6pyNXenbR2Q","colab_type":"code","outputId":"d0dee262-3e5d-4a10-9644-19e52218fe3c","executionInfo":{"status":"error","timestamp":1561972700739,"user_tz":180,"elapsed":57611,"user":{"displayName":"Pedro Diniz","photoUrl":"","userId":"09876185426137863871"}},"colab":{"base_uri":"https://localhost:8080/","height":614}},"source":["# Definindo otimizador, função de perda e métrica de eficiência. \n","from keras.optimizers import Adam\n","adamOptimizer = Adam(lr=0.001)\n","\n","model.compile( optimizer=adamOptimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'] )\n","\n","\n","# Efetuando o treinamento de 10 épocas com o dataset de treino e validando no dataset de validação\n","history = model.fit( x=x_train, y=y_train, validation_data=(x_val,y_val), epochs=10, batch_size=16, shuffle=False )\n","\n","\n","# Plotando o histórico de treino\n","\n","# Histórico de acurácia\n","pyplot.plot(history.history['acc'])\n","pyplot.plot(history.history['val_acc'])\n","pyplot.title('Acurácia do modelo no treino e validação')\n","pyplot.ylabel('Acurácia')\n","pyplot.xlabel('Época')\n","pyplot.legend(['Treino', 'Validação'], loc='upper left')\n","pyplot.show()\n","\n","# Histórico da função de perda\n","pyplot.plot(history.history['loss'])\n","pyplot.plot(history.history['val_loss'])\n","pyplot.title('Perda do modelo no treino e validação')\n","pyplot.ylabel('Perda')\n","pyplot.xlabel('Época')\n","pyplot.legend(['Treino', 'Validação'], loc='upper left')\n","pyplot.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["W0701 09:17:28.483190 140211596105600 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0701 09:17:28.494960 140211596105600 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","W0701 09:17:28.602961 140211596105600 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 45000 samples, validate on 15000 samples\n","Epoch 1/10\n","45000/45000 [==============================] - 26s 577us/step - loss: 0.2147 - acc: 0.9345 - val_loss: 0.0556 - val_acc: 0.9835\n","Epoch 2/10\n","45000/45000 [==============================] - 25s 551us/step - loss: 0.0842 - acc: 0.9746 - val_loss: 0.0471 - val_acc: 0.9872\n","Epoch 3/10\n"," 1728/45000 [>.............................] - ETA: 23s - loss: 0.0768 - acc: 0.9780"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-090a3d1645b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Efetuando o treinamento de 10 épocas com o dataset de treino e validando no dataset de validação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"XLo3Uz5vgvao","colab_type":"text"},"source":["# 5. Avaliando a CNN treinada\n","Finalmente, podemos avaliar o modelo treinado usando o dataset de teste"]},{"cell_type":"code","metadata":{"id":"tfgzKfNFgxmB","colab_type":"code","colab":{}},"source":["# Avaliando a CNN treinada\n","score = model.evaluate(x_test, y_test)\n","\n","print( '\\nPerda:{:.3f}\\nAcurácia:{}'.format( score[0], score[1] ) )\n","\n","\n","# Imprimindo uma imagem de exemplo\n","image_index = 4444\n","pyplot.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n","\n","# Predizendo o dígito dessa imagem\n","pred = model.predict( x_test[image_index].reshape(1, 28, 28, 1) )\n","print( '\\nO valor predito é:', pred.argmax() )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Atns315AVpy","colab_type":"text"},"source":["# 6. Arquiteturas\n","\n","No decorrer dos anos, arquiteturas de CNNs cada vez mais eficientes foram criadas. Vamos listar algumas delas e explicar suas melhorias.\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"Top arquiteturas\" />\n","      <center>\n","          <img  alt=\"Top arquiteturas\" src=\"https://www.researchgate.net/profile/Gustav_Von_Zitzewitz/publication/324476862/figure/fig7/AS:614545865310213@1523530560584/Winner-results-of-the-ImageNet-large-scale-visual-recognition-challenge-LSVRC-of-the.png\" width=\"70%\" >\n","          <figcaption> Arquiteturas campeãs do ImageNet.</figcaption>\n","      </center>\n","</figure>\n","\n","## 6.1 LeNet\n","\n","A LeNet foi a primeira CNN criada. Ela foi utilizada para reconhecimento de dígitos em documentos e praticamente não cometia erro. A sua principal vantagem em relação aos demais algoritmos era a capacidade de encontrar características por conta própria, como toda CNN atualmente. Apesar dessa grande e inovadora habilidade e dos excelentes resultados alcançados, a LeNet não se popularizou devido ao alto custo computacional: afinal, ela foi criada em 1998!\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"LeNet\" />\n","      <center>\n","          <img  alt=\"Arquitetura LeNet\" src=\"https://miro.medium.com/max/1000/1*1TI1aGBZ4dybR6__DI9dzA.png\" width=\"80%\" >\n","          <figcaption> Arquitetura LeNet. Fonte: <cite data-cite=\"LeCun1998\"><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\">(LeCun et al, 1998)</a></cite>\n","         </figcaption>\n","      </center>\n","</figure>\n","\n","## 6.2 AlexNet\n","\n","Quase 15 anos após a criação da LeNet, quando os computadores já tinham avançado o suficiente, a AlexNet foi utilizada no desafio ImageNet. Ela logo parou a comunidade acadêmica ao superar o erro do campeão anterior em mais de 10%. Assim iniciou-se a revolução do Deep Learning e, a partir daí, todos os anos uma nova CNN vencia o ImageNet com resultados ainda mais incríveis. \n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"AlexNet\" />\n","      <center>\n","          <img  alt=\"Arquitetura AlexNet\" src=\"https://cdn-images-1.medium.com/freeze/max/1000/1*wzflNwJw9QkjWWvTosXhNw.png?q=20\" width=\"75%\" >\n","          <figcaption> Arquitetura AlexNet. Fonte: <cite data-cite=\"Krizhevsky2012\"><a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\">(Krizhevsky et al, 2012)</a></cite>\n","         </figcaption>\n","      </center>\n","</figure>\n","\n","A AlexNet propôs diversas melhorias em relação a LeNet. Primeiro, ela usa a função de ativação ReLU no lugar da Tangente Hiperbólica, que é 6 vezes mais rápida. Também propôs o uso de Dropout como regularização, evitando Overfitting. Por fim, utilizou Max Pooling no lugar de Avarage Pooling, obtendo melhores resultados. \n","\n","No ano seguinte, uma nova versão da AlexNet, contendo apenas ajustes nos hiperparâmetros, também venceu a ImageNet. \n","\n","## 6.3 VGG\n","\n","A VGG foi a primeira arquitetura a superar a AlexNet propondo uma rede mais profunda: ela possui 19 camadas enquanto a AlexNet possui apenas 8! Desde então, redes cada vez mais profundas foram utilizadas para vencer a ImageNet.\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"VGG\" />\n","      <center>\n","          <img  alt=\"Arquitetura VGG\" src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" width=\"60%\" >\n","          <figcaption> Arquitetura VGG. Fonte: <cite data-cite=\"Simonyan2014\"><a href=\"https://arxiv.org/pdf/1409.1556.pdf\">(Simonyan et al, 2014)</a></cite>\n","         </figcaption>\n","      </center>\n","</figure>\n","\n","## 6.4 GoogLeNet\n","\n","Até o momento, as arquiteturas vencedoras apenas adicionavam mais e mais camadas. Em contrapartida, isso aumentava o custo computacional. A GoogLeNet, também conhecida como Inception, propôs diversos e novos truques para melhorar não apenas os resultados, mas também os tempos de treino e inferência.\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"GoogLeNet\" />\n","      <center>\n","          <img  alt=\"Arquitetura GoogLeNet\" src=\"https://miro.medium.com/max/2400/1*ZFPOSAted10TPd3hBQU8iQ.png\" width=\"100%\" >\n","          <figcaption> Arquitetura GoogLeNet. Fonte: <cite data-cite=\"Szegedy2014\"><a href=\"https://arxiv.org/pdf/1409.4842v1.pdf\">(Szegedy et al, 2014)</a></cite>\n","         </figcaption>\n","      </center>\n","</figure>\n","\n","### 6.4.1 Módulo Inception\n","\n","A principal melhoria proposta pela GoogLeNet é o módulo Inception. Nele, executa-se um conjunto de convoluções paralelamente com filtros de diferentes tamanhos. Depois, os mapas de características de cada convolução são concatenados em um único mapa. \n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"Módulo Inception\" />\n","      <center>\n","          <img  alt=\"Módulo Inception\" src=\"https://cdn-images-1.medium.com/max/800/1*DKjGRDd_lJeUfVlY50ojOA.png\" width=\"50%\" >\n","          <figcaption> Módulo Inception. Fonte: <cite data-cite=\"Szegedy2014\"><a href=\"https://arxiv.org/pdf/1409.4842v1.pdf\">(Szegedy et al, 2014)</a></cite>\n","         </figcaption>\n","      </center>\n","</figure>\n","\n","Quando um filtro é pequeno ele se concentra em informações locais numa convolução. Analogamente, filtros grandes se concentram em informações mais globais. A ideia do módulo Inception é combinar informações locais e globais já que não se sabe qual importa mais em cada imagem. Isso melhora o resultado final da rede.\n","\n","### 6.4.2 Filtros 1x1\n","\n","Como já explicado, os mapas de características são concatenados no final do módulo Inception. O mapa resultante pode ficar muito grande, deixando operações posteriores custosas e lentas. \n","\n","Para resolver esse problema, a GoogLeNet propõe reduzir os mapas antes de concatená-los. Para isso, ela os convolui com uma quantidade pequena de filtros 1x1, resultando em novos mapas pequenos (já que aplicou-se poucos filtros). Então, um mapa de 28x28x256 convoluído por 64 filtros 1x1 resultaria em um mapa 28x28x64, por exemplo.\n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"Módulo Inception (dimensionalidade reduzida)\" />\n","      <center>\n","          <img  alt=\"Módulo Inception (dimensionalidade reduzida)\" src=\"https://cdn-images-1.medium.com/max/800/1*U_McJnp7Fnif-lw9iIC5Bw.png\" width=\"50%\" >\n","          <figcaption> Módulo Inception (dimensionalidade reduzida). Fonte: <cite data-cite=\"Szegedy2014\"><a href=\"https://arxiv.org/pdf/1409.4842v1.pdf\">(Szegedy et al, 2014)</a></cite>\n","         </figcaption>\n","      </center>\n","</figure>\n","\n","### 6.4.3 Classificadores auxiliares\n","\n","Normalmente, a medida que adicionamos camadas na CNN, os resultados vão melhorando. No entanto, a partir de certa profundidade, os resultados pioram muito a cada nova camada adicionada. Isso não é causado por overfitting, pois  esse aumento no erro ocorre ainda na etapa de treino. Ou seja, uma CNN rasa pode ter um erro de treino menor que o uma CNN muito profunda. Isso ocorre porque uma CNN complexa (com muitos parâmetros) é difícil de otimizar, pois o gradiente tende a desaparecer a medida que retropropaga em muitas camadas.\n","\n","A GoogLeNet é profunda, maior que a VGG, possuindo 22 camadas, e por isso possui esse defeito. Para solucionar isso, ela propõe usar classificadores auxiliares no decorrer da rede. Assim, o erro final será a soma ponderada dos erros dos classificadores auxiliares e do final.\n","\n","## 6.5 ResNet\n","\n","Como explicado, redes cada vez mais produndas tendem a ter, a partir de certa profundidade, resultados cada vez piores. Se esse problema fosse resolvido, seria possível utilizar arquiteturas cada vez maiores e assim obter melhores resultados. A GoogLeNet amenizou esse problema propondo classificadores auxiliares. A Microsoft, por sua vez, também criou a sua própria abordagem com as Residual Neural Networks (ResNet) e, assim, conseguiu usar com sucesso arquiteturas imensas que não seriam viáveis anteriormente. \n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"ResNet\" />\n","      <center>\n","          <img  alt=\"Arquitetura ResNet\" src=\"https://cdn-images-1.medium.com/max/1400/1*6hF97Upuqg_LdsqWY6n_wg.png\" width=\"100%\" >\n","          <figcaption> Arquitetura ResNet. Fonte: <cite data-cite=\"He2015\"><a href=\"https://arxiv.org/pdf/1512.03385.pdf\">(He et al, 2015)</a></cite>\n","         </figcaption>\n","      </center>\n","</figure>\n","\n","### 6.5.1 Conexões residuais\n","\n","A idéia da ResNet é que camadas convolucionais alimentem “residualmente” camadas posteriores. Para isso, ela usa conexões paralelas que pulam camadas denominadas de conexões residuais. Uma conexão residual simplesmente copia um mapa de características de uma camada e concatena em outro numa camada posterior. Dessa forma, os erros retropropagados podem alcançar facilmente às primeiras camadas da rede, resolvendo o problema de resultados ruins em arquitetura grandes. \n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"Conexão residual\" />\n","      <center>\n","          <img  alt=\"Conexão residual\" src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/residual_building_block.png\" width=\"35%\" >\n","          <figcaption> Conexão residual. Fonte: <cite data-cite=\"He2015\"><a href=\"https://arxiv.org/pdf/1512.03385.pdf\">(He et al, 2015)</a></cite>\n","         </figcaption>\n","      </center>\n","</figure>\n","\n","### 6.5.2 Filtros 1x1\n","\n","Como as conexões residuais concatenam mapas de características, eles podem ficar muito grandes e, assim, caros de operar. Para diminui-los, em algumas ResNets utiliza-se também as convoluções de poucos filtros 1x1, propostos pela GoogLeNet. \n","\n","<figure style=\"text-align: center;\" >\n","      <a name=\"Conexão residual (dimensionalidade reduzida)\" />\n","      <center>\n","          <img  alt=\"Conexão residual (dimensionalidade reduzida)\" src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/residual_bottleneck.png\" width=\"30%\" >\n","          <figcaption> Conexão residual (dimensionalidade reduzida). Fonte: <cite data-cite=\"He2015\"><a href=\"https://arxiv.org/pdf/1512.03385.pdf\">(He et al, 2015)</a></cite>\n","         </figcaption>\n","      </center>\n","</figure>\n","\n","### 6.5.3 Grande profundidade\n","\n","Os autores mostraram que ResNets extremamente profundas são fáceis de otimizar. Assim, ao contrário de CNNs comuns (que simplesmente empilham camadas), elas podem se beneficiar de resultados melhores simplesmente aumentando a profundidade. Utilizando esse preceito, a ResNet conseguiu superar os resultados da GoogLeNet utilizando uma arquitetura de 152 camadas! Surpreendentemente, ela é 8x mais rápida que a GoogLeNet, que possui apenas 22 camadas.\n","\n","## 6.6 Arquiteturas em prática\n","\n","Agora que já falamos sobre as arquiteturas mais famosas, vamos ver como elas se saem na prática. Essas CNNs foram treinadas na ImageNet, que possui milhões de imagens, e por isso não seria viável treiná-las agora. Ao invés disso, vamos baixar as redes já treinadas e usá-las apenas para classificação."]},{"cell_type":"code","metadata":{"id":"ueseE1Yv8Bfr","colab_type":"code","colab":{}},"source":["# Carregando modelos VGG, GoogLeNet e ResNet50 pré-treinados no ImageNet\n","from keras.applications import vgg19, inception_v3, resnet50\n","\n","vggModel = vgg19.VGG19(weights='imagenet')\n","googLeNetModel = inception_v3.InceptionV3(weights='imagenet')\n","resNetModel = resnet50.ResNet50(weights='imagenet')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nAb1d9wrNoQe","colab_type":"text"},"source":["Agora que baixamos as redes pré-treinadas, vamos usá-as para classificar uma imagem e analisar os resultados"]},{"cell_type":"code","metadata":{"id":"qE0SbwvoEiak","colab_type":"code","colab":{}},"source":["# Carregando imagem de teste\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","from matplotlib import pyplot\n","\n","filename = 'https://www.learnopencv.com/wp-content/uploads/2017/12/cat.jpg'\n","response = requests.get(filename)\n","\n","original = Image.open(BytesIO(response.content))\n","\n","pyplot.imshow(original)\n","pyplot.show()\n","\n","\n","# Preprocessando para ser entrada das redes\n","from keras.preprocessing.image import img_to_array\n","import numpy\n","\n","# Redimensionando\n","imageVGG = original.resize((224, 224), Image.NEAREST)\n","imageGoogLeNet = original.resize((299, 299), Image.NEAREST)\n","imageResNet = original.resize((224, 224), Image.NEAREST)\n","\n","# Convertendo pra numpy array (height, width, channel)\n","numpyImageVGG = img_to_array(imageVGG)\n","numpyImageGoogLeNet = img_to_array(imageGoogLeNet)\n","numpyImageResNet = img_to_array(imageResNet)\n"," \n","# Convertendo para batch de imagens (batchsize, height, width, channels)\n","imageBatchVGG = numpy.expand_dims(numpyImageVGG, axis=0)\n","imageBatchGoogLeNet = numpy.expand_dims(numpyImageGoogLeNet, axis=0)\n","imageBatchResNet = numpy.expand_dims(numpyImageResNet, axis=0)\n","\n","# Preprocessamento específico de cada rede\n","processedImageVGG = vgg19.preprocess_input(imageBatchVGG.copy())\n","processedImageGoogLeNet = inception_v3.preprocess_input(imageBatchGoogLeNet.copy())\n","processedImageResNet = resnet50.preprocess_input(imageBatchResNet.copy())\n","\n","\n","# Predizendo\n","predictionsVGG = vggModel.predict(processedImageVGG)\n","predictionsGoogLeNet = googLeNetModel.predict(processedImageGoogLeNet)\n","predictionsResNet = resNetModel.predict(processedImageResNet)\n","\n","\n","# Pegando 5 classes de maior probabilidade pra cada rede\n","from keras.applications.imagenet_utils import decode_predictions\n","\n","labelVGG = decode_predictions(predictionsVGG)\n","labelGoogLeNet = decode_predictions(predictionsGoogLeNet)\n","labelResNet = decode_predictions(predictionsResNet)\n","\n","print('Valores preditos pela VGG 16:', labelVGG[0]) \n","print('Valores preditos pela GoogLeNet v3:', labelGoogLeNet[0]) \n","print('Valores preditos pela ResNet 50:', labelResNet[0]) "],"execution_count":0,"outputs":[]}]}